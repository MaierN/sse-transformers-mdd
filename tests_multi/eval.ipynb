{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric, load_from_disk\n",
    "from transformers import (\n",
    "    RobertaTokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_num_threads(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(\"Salesforce/codet5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"/data/nicolasmaier/model/codet5-merged-1/checkpoint-9000\")\n",
    "model.eval()\n",
    "\n",
    "print(\"ok\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: Dataset({\n",
      "    features: ['code', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 6000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset_test = load_from_disk(\"/data/nicolasmaier/dataset/hf_merged_test_1\")\n",
    "print(\"test:\", dataset_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['code', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 100\n",
      "})\n",
      "   "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08c34613bc7942a6977cc05b569e84db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go\n",
      "ok1\n",
      "ok\n",
      "ok2\n",
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57ac1dbd966e48569977e45e5b944835",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go\n",
      "ok1\n",
      "ok\n",
      "ok2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [52], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m xyz \u001b[39m=\u001b[39m dataset_test\u001b[39m.\u001b[39mselect(\u001b[39mrange\u001b[39m(\u001b[39m100\u001b[39m))\n\u001b[1;32m     29\u001b[0m \u001b[39mprint\u001b[39m(xyz)\n\u001b[0;32m---> 30\u001b[0m dataset_results \u001b[39m=\u001b[39m dataset_test\u001b[39m.\u001b[39;49mselect(\u001b[39mrange\u001b[39;49m(\u001b[39m4\u001b[39;49m))\u001b[39m.\u001b[39;49mmap(\n\u001b[1;32m     31\u001b[0m     compute_predictions, batched\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m, num_proc\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m\n\u001b[1;32m     32\u001b[0m )\n",
      "File \u001b[0;32m~/workspace/sse-transformers-mdd/venv/lib/python3.8/site-packages/datasets/arrow_dataset.py:2685\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   2680\u001b[0m         \u001b[39massert\u001b[39;00m (\n\u001b[1;32m   2681\u001b[0m             \u001b[39mlen\u001b[39m(results) \u001b[39m==\u001b[39m nb_of_missing_shards\n\u001b[1;32m   2682\u001b[0m         ), \u001b[39m\"\u001b[39m\u001b[39mThe number of missing cached shards needs to correspond to the number of `_map_single` we\u001b[39m\u001b[39m'\u001b[39m\u001b[39mre running\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2684\u001b[0m         \u001b[39mfor\u001b[39;00m index, async_result \u001b[39min\u001b[39;00m results\u001b[39m.\u001b[39mitems():\n\u001b[0;32m-> 2685\u001b[0m             transformed_shards[index] \u001b[39m=\u001b[39m async_result\u001b[39m.\u001b[39;49mget()\n\u001b[1;32m   2687\u001b[0m \u001b[39massert\u001b[39;00m (\n\u001b[1;32m   2688\u001b[0m     transformed_shards\u001b[39m.\u001b[39mcount(\u001b[39mNone\u001b[39;00m) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m   2689\u001b[0m ), \u001b[39m\"\u001b[39m\u001b[39mAll shards have to be defined Datasets, none should still be missing.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2691\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mConcatenating \u001b[39m\u001b[39m{\u001b[39;00mnum_proc\u001b[39m}\u001b[39;00m\u001b[39m shards\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/workspace/sse-transformers-mdd/venv/lib/python3.8/site-packages/multiprocess/pool.py:765\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 765\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    766\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mready():\n\u001b[1;32m    767\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m\n",
      "File \u001b[0;32m~/workspace/sse-transformers-mdd/venv/lib/python3.8/site-packages/multiprocess/pool.py:762\u001b[0m, in \u001b[0;36mApplyResult.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwait\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 762\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_event\u001b[39m.\u001b[39;49mwait(timeout)\n",
      "File \u001b[0;32m/usr/lib/python3.8/threading.py:558\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    556\u001b[0m signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flag\n\u001b[1;32m    557\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 558\u001b[0m     signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cond\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    559\u001b[0m \u001b[39mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m/usr/lib/python3.8/threading.py:302\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    301\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 302\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    303\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    304\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def compute_predictions(examples):\n",
    "    print(\"go\")\n",
    "    orig_input = [tokenizer.decode(x[1:-1]) for x in examples[\"input_ids\"]]\n",
    "    print(\"ok1\")\n",
    "    orig_output = [tokenizer.decode(x[1:-1]) for x in examples[\"labels\"]]\n",
    "    print(\"ok\")\n",
    "\n",
    "    model_input = tokenizer(\n",
    "        orig_input, padding=True, truncation=True, return_tensors=\"pt\"\n",
    "    )\n",
    "    print(\"ok2\")\n",
    "\n",
    "    outputs = model.generate(\n",
    "        model_input.input_ids,\n",
    "        attention_mask=model_input.attention_mask,\n",
    "        num_beams=10,\n",
    "        max_length=510,\n",
    "        # do_sample=True,\n",
    "        temperature=0.3,\n",
    "        top_k=50,\n",
    "        top_p=0.8,\n",
    "    )\n",
    "    print(\"ok3\")\n",
    "\n",
    "    return {\"prediction\": outputs}\n",
    "\n",
    "\n",
    "xyz = dataset_test.select(range(100))\n",
    "print(xyz)\n",
    "dataset_results = dataset_test.select(range(4)).map(\n",
    "    compute_predictions, batched=True, batch_size=2, num_proc=2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "int() argument must be a string, a bytes-like object or a number, not 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mprint\u001b[39m(compute_predictions(dataset_test\u001b[39m.\u001b[39;49mselect(\u001b[39mrange\u001b[39;49m(\u001b[39m10\u001b[39;49m))))\n",
      "Cell \u001b[0;32mIn [22], line 2\u001b[0m, in \u001b[0;36mcompute_predictions\u001b[0;34m(examples)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_predictions\u001b[39m(examples):\n\u001b[0;32m----> 2\u001b[0m     orig_input \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39;49mdecode([x[\u001b[39m1\u001b[39;49m:\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m] \u001b[39mfor\u001b[39;49;00m x \u001b[39min\u001b[39;49;00m examples[\u001b[39m\"\u001b[39;49m\u001b[39minput_ids\u001b[39;49m\u001b[39m\"\u001b[39;49m]])\n\u001b[1;32m      3\u001b[0m     orig_output \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mdecode([x[\u001b[39m1\u001b[39m:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m examples[\u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m]])\n\u001b[1;32m      5\u001b[0m     model_input \u001b[39m=\u001b[39m tokenizer(orig_input)\n",
      "File \u001b[0;32m~/workspace/sse-transformers-mdd/venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3436\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3433\u001b[0m \u001b[39m# Convert inputs to python lists\u001b[39;00m\n\u001b[1;32m   3434\u001b[0m token_ids \u001b[39m=\u001b[39m to_py_obj(token_ids)\n\u001b[0;32m-> 3436\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_decode(\n\u001b[1;32m   3437\u001b[0m     token_ids\u001b[39m=\u001b[39;49mtoken_ids,\n\u001b[1;32m   3438\u001b[0m     skip_special_tokens\u001b[39m=\u001b[39;49mskip_special_tokens,\n\u001b[1;32m   3439\u001b[0m     clean_up_tokenization_spaces\u001b[39m=\u001b[39;49mclean_up_tokenization_spaces,\n\u001b[1;32m   3440\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   3441\u001b[0m )\n",
      "File \u001b[0;32m~/workspace/sse-transformers-mdd/venv/lib/python3.8/site-packages/transformers/tokenization_utils.py:931\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, spaces_between_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m    921\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_decode\u001b[39m(\n\u001b[1;32m    922\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    923\u001b[0m     token_ids: List[\u001b[39mint\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    928\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[1;32m    929\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decode_use_source_tokenizer \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39muse_source_tokenizer\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m--> 931\u001b[0m     filtered_tokens \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_ids_to_tokens(token_ids, skip_special_tokens\u001b[39m=\u001b[39;49mskip_special_tokens)\n\u001b[1;32m    933\u001b[0m     \u001b[39m# To avoid mixing byte-level and unicode for byte-level BPT\u001b[39;00m\n\u001b[1;32m    934\u001b[0m     \u001b[39m# we need to build string separately for added tokens and byte-level tokens\u001b[39;00m\n\u001b[1;32m    935\u001b[0m     \u001b[39m# cf. https://github.com/huggingface/transformers/issues/1133\u001b[39;00m\n\u001b[1;32m    936\u001b[0m     sub_texts \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/workspace/sse-transformers-mdd/venv/lib/python3.8/site-packages/transformers/tokenization_utils.py:906\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.convert_ids_to_tokens\u001b[0;34m(self, ids, skip_special_tokens)\u001b[0m\n\u001b[1;32m    904\u001b[0m tokens \u001b[39m=\u001b[39m []\n\u001b[1;32m    905\u001b[0m \u001b[39mfor\u001b[39;00m index \u001b[39min\u001b[39;00m ids:\n\u001b[0;32m--> 906\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mint\u001b[39;49m(index)\n\u001b[1;32m    907\u001b[0m     \u001b[39mif\u001b[39;00m skip_special_tokens \u001b[39mand\u001b[39;00m index \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mall_special_ids:\n\u001b[1;32m    908\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a number, not 'list'"
     ]
    }
   ],
   "source": [
    "print(compute_predictions(dataset_test.select(range(10))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /data/nicolasmaier/dataset/hf_merged_test_1/cache-3da988711144e5ae.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>java to sequence\n",
      "private PointFeature nextFilteredDataPoint() {\n",
      "        while (origIter.hasNext()) {\n",
      "            PointFeature pointFeat = origIter.next();\n",
      "            if (filter.filter(pointFeat)) {\n",
      "                return pointFeat;\n",
      "            }\n",
      "        }\n",
      "\n",
      "        return null;\n",
      "    }</s>\n",
      "<s>{\"title\": \"nextFilteredDataPoint()\", \"sequence\": [{\"type\": \"methodInvocation\", \"to\": [\"origIter\"], \"method\": \"hasNext()\"}, {\"type\": \"blocks\", \"name\": \"while\", \"blocks\": [{\"guard\": \"origIter.hasNext()\", \"contents\": [{\"type\": \"methodInvocation\", \"to\": [\"origIter\"], \"method\": \"next()\"}, {\"type\": \"scopedVariable\", \"name\": \"pointFeat\"}, {\"type\": \"methodInvocation\", \"to\": [\"filter\"], \"method\": \"filter(pointFeat)\"}, {\"type\": \"blocks\", \"name\": \"if\", \"blocks\": [{\"guard\": \"filter.filter(pointFeat)\", \"contents\": [{\"type\": \"controlFlow\", \"name\": \"return\", \"value\": \"pointFeat\"}]}]}]}]}, {\"type\": \"controlFlow\", \"name\": \"return\", \"value\": \"null\"}]}</s>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(dataset_test.shuffle(21)[2][\"input_ids\"]))\n",
    "print(tokenizer.decode(dataset_test.shuffle(21)[2][\"labels\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load_metric(\"bleu\")\n",
    "print(metric_bleu.inputs_description)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, Nov 14 2022, 12:59:47) \n[GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "605e098c6041b6083d991721b9f8ec0203cba3209db372356f118eea72c915b6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
