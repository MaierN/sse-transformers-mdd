{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import load_from_disk, load_dataset\n",
    "from transformers import RobertaTokenizer\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(\"Salesforce/codet5-small\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: code_search_net/all\n",
      "Found cached dataset code_search_net (/data/nicolasmaier/huggingface_cache/code_search_net/all/1.0.0/80a244ab541c6b2125350b764dc5c2b715f65f00de7a56107a28915fac173a27)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b1d5023735342c4854fa4ec0e1d0fb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url'],\n",
      "        num_rows: 1880853\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url'],\n",
      "        num_rows: 100529\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url'],\n",
      "        num_rows: 89154\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset_csn = load_dataset(\"code_search_net\")\n",
    "print(dataset_csn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_csn = dataset_csn.filter(\n",
    "    lambda example: example[\"language\"] in [\"java\", \"python\", \"javascript\"],\n",
    "    num_proc=64,\n",
    ")\n",
    "\n",
    "print(dataset_csn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_examples(example):\n",
    "    code = example[\"func_code_string\"]\n",
    "    doc = example[\"func_documentation_string\"]\n",
    "\n",
    "    new_code = code.replace(doc, \"\")\n",
    "\n",
    "    return {\"code\": new_code}\n",
    "\n",
    "\n",
    "dataset_csn = dataset_csn.map(\n",
    "    preprocess_examples,\n",
    "    num_proc=64,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_examples(examples):\n",
    "    codes = examples[\"code\"]\n",
    "    languages = examples[\"language\"]\n",
    "\n",
    "    contents = [\n",
    "        f\"{language} to summary\\n{code}\" for code, language in zip(codes, languages)\n",
    "    ]\n",
    "    model_inputs = tokenizer(contents)\n",
    "\n",
    "    labels = tokenizer(examples[\"func_documentation_string\"])\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "dataset_csn = dataset_csn.map(\n",
    "    preprocess_examples,\n",
    "    batched=True,\n",
    "    batch_size=100,\n",
    "    num_proc=64,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url', 'code', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 990518\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url', 'code', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 55568\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url', 'code', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 46688\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset_csn)\n",
    "dataset_csn.save_to_disk(\"/data/nicolasmaier/dataset/hf_csn_1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 505\n",
    "\n",
    "dataset_csn_filtered = dataset_csn.filter(\n",
    "    lambda example: len(example[\"input_ids\"]) <= MAX_LENGTH, num_proc=64\n",
    ")\n",
    "print(dataset_csn_filtered)\n",
    "dataset_csn_filtered = dataset_csn_filtered.filter(\n",
    "    lambda example: len(example[\"labels\"]) <= MAX_LENGTH,\n",
    "    num_proc=64,\n",
    ")\n",
    "print(dataset_csn_filtered)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url', 'code', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 909090\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url', 'code', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 50975\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url', 'code', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 42644\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /data/nicolasmaier/huggingface_cache/code_search_net/all/1.0.0/80a244ab541c6b2125350b764dc5c2b715f65f00de7a56107a28915fac173a27/cache-7c9f33ff27bec466.arrow\n",
      "Loading cached processed dataset at /data/nicolasmaier/huggingface_cache/code_search_net/all/1.0.0/80a244ab541c6b2125350b764dc5c2b715f65f00de7a56107a28915fac173a27/cache-3e07b87b41575efc.arrow\n",
      "Loading cached processed dataset at /data/nicolasmaier/huggingface_cache/code_search_net/all/1.0.0/80a244ab541c6b2125350b764dc5c2b715f65f00de7a56107a28915fac173a27/cache-6d5b0294cb4c1912.arrow\n"
     ]
    }
   ],
   "source": [
    "print(dataset_csn_filtered)\n",
    "dataset_csn_filtered.save_to_disk(\"/data/nicolasmaier/dataset/hf_clean_csn_1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['code', 'contents', 'xmi', 'originalLine', 'input_ids', 'attention_mask', 'seq', 'labels'],\n",
      "        num_rows: 454273\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['code', 'contents', 'xmi', 'originalLine', 'input_ids', 'attention_mask', 'seq', 'labels'],\n",
      "        num_rows: 15326\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['code', 'contents', 'xmi', 'originalLine', 'input_ids', 'attention_mask', 'seq', 'labels'],\n",
      "        num_rows: 26902\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset_seq = load_from_disk(\"/data/nicolasmaier/dataset/hf_seq_dataset_3\")\n",
    "print(dataset_seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_examples(examples):\n",
    "    codes = examples[\"code\"]\n",
    "\n",
    "    contents = [f\"java to sequence\\n{code}\" for code in codes]\n",
    "    model_inputs = tokenizer(contents)\n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "dataset_seq = dataset_seq.map(\n",
    "    preprocess_examples,\n",
    "    batched=True,\n",
    "    batch_size=100,\n",
    "    num_proc=64,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['code', 'contents', 'xmi', 'originalLine', 'input_ids', 'attention_mask', 'seq', 'labels'],\n",
      "        num_rows: 454273\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['code', 'contents', 'xmi', 'originalLine', 'input_ids', 'attention_mask', 'seq', 'labels'],\n",
      "        num_rows: 15326\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['code', 'contents', 'xmi', 'originalLine', 'input_ids', 'attention_mask', 'seq', 'labels'],\n",
      "        num_rows: 26902\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset_seq)\n",
    "dataset_seq.save_to_disk(\"/data/nicolasmaier/dataset/hf_seq_1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 505\n",
    "\n",
    "dataset_seq_filtered = dataset_seq.filter(\n",
    "    lambda example: len(example[\"input_ids\"]) <= MAX_LENGTH, num_proc=64\n",
    ")\n",
    "print(dataset_seq_filtered)\n",
    "dataset_seq_filtered = dataset_seq_filtered.filter(\n",
    "    lambda example: len(example[\"labels\"]) <= MAX_LENGTH,\n",
    "    num_proc=64,\n",
    ")\n",
    "print(dataset_seq_filtered)\n",
    "dataset_seq_filtered = dataset_seq_filtered.filter(\n",
    "    lambda example: len(example[\"seq\"]) > 10,\n",
    "    num_proc=64,\n",
    ")\n",
    "print(dataset_seq_filtered)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['code', 'contents', 'xmi', 'originalLine', 'input_ids', 'attention_mask', 'seq', 'labels'],\n",
      "        num_rows: 366219\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['code', 'contents', 'xmi', 'originalLine', 'input_ids', 'attention_mask', 'seq', 'labels'],\n",
      "        num_rows: 13021\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['code', 'contents', 'xmi', 'originalLine', 'input_ids', 'attention_mask', 'seq', 'labels'],\n",
      "        num_rows: 21560\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e79f05735a4d41c796a9fe7773794a51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/367 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "390b8ab3b7d74f4083b005b48b7aa46c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/14 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c126de1701db476abf4d30245d451dc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/22 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(dataset_seq_filtered)\n",
    "dataset_seq_filtered.save_to_disk(\"/data/nicolasmaier/dataset/hf_clean_seq_1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['code', 'contents', 'xmi', 'originalLine', 'input_ids', 'attention_mask', 'seq', 'labels'],\n",
      "        num_rows: 366219\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['code', 'contents', 'xmi', 'originalLine', 'input_ids', 'attention_mask', 'seq', 'labels'],\n",
      "        num_rows: 13021\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['code', 'contents', 'xmi', 'originalLine', 'input_ids', 'attention_mask', 'seq', 'labels'],\n",
      "        num_rows: 21560\n",
      "    })\n",
      "})\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url', 'code', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 909090\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url', 'code', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 50975\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url', 'code', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 42644\n",
      "    })\n",
      "})\n",
      "<s>java to sequence\n",
      "protected final void bindIndexed(ConfigurationPropertyName name, Bindable<?> target,\n",
      "\t\t\tAggregateElementBinder elementBinder, ResolvableType aggregateType,\n",
      "\t\t\tResolvableType elementType, IndexedCollectionSupplier result) {\n",
      "\t\tfor (ConfigurationPropertySource source : getContext().getSources()) {\n",
      "\t\t\tbindIndexed(source, name, target, elementBinder, result, aggregateType,\n",
      "\t\t\t\t\telementType);\n",
      "\t\t\tif (result.wasSupplied() && result.get()!= null) {\n",
      "\t\t\t\treturn;\n",
      "\t\t\t}\n",
      "\t\t}\n",
      "\t}</s>\n",
      "<s>{\"title\": \"bindIndexed(name, target, elementBinder, aggregateType, elementType, result)\", \"sequence\": [{\"type\": \"methodInvocation\", \"to\": [], \"method\": \"getContext()\"}, {\"type\": \"methodInvocation\", \"to\": \"### unk\", \"method\": \"getSources()\"}, {\"type\": \"blocks\", \"name\": \"for\", \"blocks\": [{\"guard\": \"source in [...].getSources()\", \"contents\": [{\"type\": \"scopedVariable\", \"name\": \"source\"}, {\"type\": \"methodInvocation\", \"to\": [], \"method\": \"bindIndexed(source, name, target, elementBinder, result, aggregateType, elementType)\"}, {\"type\": \"methodInvocation\", \"to\": [\"result\"], \"method\": \"wasSupplied()\"}, {\"type\": \"methodInvocation\", \"to\": [\"result\"], \"method\": \"get()\"}, {\"type\": \"blocks\", \"name\": \"if\", \"blocks\": [{\"guard\": \"result.wasSupplied() && result.get()!= null\", \"contents\": [{\"type\": \"controlFlow\", \"name\": \"return\", \"value\": null}]}]}]}]}]}</s>\n",
      "<s>python to summary\n",
      "def export_ruptures_csv(ekey, dstore):\n",
      "    \"\"\"\n",
      "    \n",
      "    \"\"\"\n",
      "    oq = dstore['oqparam']\n",
      "    if'scenario' in oq.calculation_mode:\n",
      "        return []\n",
      "    dest = dstore.export_path('ruptures.csv')\n",
      "    header = ('rupid multiplicity mag centroid_lon centroid_lat '\n",
      "              'centroid_depth trt strike dip rake boundary').split()\n",
      "    rows = []\n",
      "    for rgetter in gen_rupture_getters(dstore):\n",
      "        rups = rgetter.get_ruptures()\n",
      "        rup_data = calc.RuptureData(rgetter.trt, rgetter.rlzs_by_gsim)\n",
      "        for r in rup_data.to_array(rups):\n",
      "            rows.append(\n",
      "                (r['rup_id'], r['multiplicity'], r['mag'],\n",
      "                 r['lon'], r['lat'], r['depth'],\n",
      "                 rgetter.trt, r['strike'], r['dip'], r['rake'],\n",
      "                 r['boundary']))\n",
      "    rows.sort()  # by rupture serial\n",
      "    comment = 'investigation_time=%s, ses_per_logic_tree_path=%s' % (\n",
      "        oq.investigation_time, oq.ses_per_logic_tree_path)\n",
      "    writers.write_csv(dest, rows, header=header, sep='\\t', comment=comment)\n",
      "    return [dest]</s>\n",
      "<s>:param ekey: export key, i.e. a pair (datastore key, fmt)\n",
      "    :param dstore: datastore object</s>\n"
     ]
    }
   ],
   "source": [
    "print(dataset_seq_filtered)\n",
    "print(dataset_csn_filtered)\n",
    "\n",
    "print(tokenizer.decode(dataset_seq_filtered[\"train\"][0][\"input_ids\"]))\n",
    "print(tokenizer.decode(dataset_seq_filtered[\"train\"][0][\"labels\"]))\n",
    "print(tokenizer.decode(dataset_csn_filtered[\"train\"][0][\"input_ids\"]))\n",
    "print(tokenizer.decode(dataset_csn_filtered[\"train\"][0][\"labels\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, Nov 14 2022, 12:59:47) \n[GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "605e098c6041b6083d991721b9f8ec0203cba3209db372356f118eea72c915b6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
